{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iatYWl9wNCL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# API Key and Custom Search Engine ID (replace with yours)\n",
        "API_KEY = '**********'\n",
        "CSE_ID = '***********'\n",
        "\n",
        "def google_search(query, api_key, cse_id, start_index=1):\n",
        "    \"\"\"Perform a Google Search using the Custom Search API.\"\"\"\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=query, cx=cse_id, start=start_index).execute()\n",
        "    return res.get('items', [])\n",
        "\n",
        "def collect_data(query, num_results=100):\n",
        "    \"\"\"Collect data from search results.\"\"\"\n",
        "    data = []\n",
        "    start_index = 1\n",
        "    while len(data) < num_results:\n",
        "        try:\n",
        "            results = google_search(query, API_KEY, CSE_ID, start_index)\n",
        "            if not results:\n",
        "                break\n",
        "            for item in results:\n",
        "                data.append({\n",
        "                    'Title': item.get('title'),\n",
        "                    'Link': item.get('link'),\n",
        "                    'Snippet': item.get('snippet'),\n",
        "                    'DisplayLink': item.get('displayLink')\n",
        "                })\n",
        "            start_index += 10  # API fetches 10 results per page\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "    return data[:num_results]\n",
        "\n",
        "def enrich_data(df):\n",
        "    \"\"\"Add extra columns to the dataframe.\"\"\"\n",
        "    df['Event Type'] = df['Title'].apply(\n",
        "        lambda x: 'Anime & Comics Cosplay Festival' if 'Anime & Comics' in x else 'Other'\n",
        "    )\n",
        "    df['Event Date'] = df['Snippet'].str.extract(r'(\\b[A-Za-z]+\\s\\d{1,2},\\s\\d{4}\\b)')\n",
        "    df['Venue'] = df['Snippet'].str.extract(r'at\\s([A-Za-z\\s]+)')\n",
        "    df['Language'] = 'English'\n",
        "    df['Price Range'] = df['Snippet'].str.extract(r'(\\$\\d+\\s?-\\s?\\$\\d+)')\n",
        "    df['Keywords'] = df['Snippet'].apply(lambda x: ', '.join(x.split()[:5]))\n",
        "    df['Region'] = 'Baku'\n",
        "    df['Category'] = 'General'\n",
        "    df['Source'] = df['DisplayLink']\n",
        "    df['Event Name'] = df['Title']\n",
        "    df['Event Details'] = df['Snippet']\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"site:tickets-az.com/en/baku/events\"\n",
        "    num_results = 100\n",
        "\n",
        "    results = collect_data(query, num_results)\n",
        "    df = pd.DataFrame(results)\n",
        "    df = enrich_data(df)\n",
        "\n",
        "    data_folder = \"data\"\n",
        "    os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "    csv_filename = os.path.join(data_folder, 'scraped_data_team_26.csv')\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    print(f\"Data collection and enrichment complete! Saved to {csv_filename}\")\n"
      ]
    }
  ]
}